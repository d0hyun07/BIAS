import pandas as pd
import numpy as np
import librosa
import librosa.display
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, BatchNormalization

# ğŸ“Œ 1ï¸âƒ£ ì˜¤ë””ì˜¤ ì¦ê°• (Data Augmentation)
def augment_audio(y, sr):
    # ëœë¤ í”¼ì¹˜ ë³€í˜• (Pitch Shifting)
    pitch_shift = np.random.uniform(-2, 2)
    y_pitch = librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_shift)

    # ëœë¤ íƒ€ì„ ìŠ¤íŠ¸ë ˆì¹­ (Time Stretching)
    stretch_rate = np.random.uniform(0.8, 1.2)
    y_stretch = librosa.effects.time_stretch(y, rate=stretch_rate)

    # ëœë¤ ì¡ìŒ ì¶”ê°€ (Adding Noise)
    noise_factor = 0.005 * np.random.randn(len(y))
    y_noise = y + noise_factor

    return [y_pitch, y_stretch, y_noise]

# ğŸ“Œ 2ï¸âƒ£ ê¸°ì¡´ `guitar_features.csv` ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("guitar_features.csv")

# ì¦ê°•ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
new_features = []
new_labels = []

for i, row in df.iterrows():
    features = row[:-1].values  # ê¸°ì¡´ íŠ¹ì§• ì‚¬ìš©
    label = row['label']
    
    # ì›ë³¸ ë°ì´í„° ì¶”ê°€
    new_features.append(features)
    new_labels.append(label)

    # ì¦ê°•ëœ ë°ì´í„° ìƒì„±
    y, sr = librosa.load(row['file_path'], sr=22050)  # íŒŒì¼ ê²½ë¡œì—ì„œ ë¡œë“œ
    for aug in augment_audio(y, sr):
        aug_features = librosa.feature.mfcc(y=aug, sr=sr, n_mfcc=13).mean(axis=1)
        new_features.append(aug_features)
        new_labels.append(label)

# ğŸ“Œ 3ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ (ë¼ë²¨ ì¸ì½”ë”© & ì •ê·œí™”)
X = np.array(new_features)
y = np.array(new_labels)

encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (80% í•™ìŠµ, 20% í…ŒìŠ¤íŠ¸)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

# ë°ì´í„° í˜•íƒœ ë³€í™˜ (CNN + LSTM ì…ë ¥ ë§ì¶”ê¸°)
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

print(f"ğŸ”¥ ë°ì´í„°ì…‹ í¬ê¸° (ì¦ê°• í¬í•¨): {X_train.shape[0]} ê°œì˜ í›ˆë ¨ ìƒ˜í”Œ, {X_test.shape[0]} ê°œì˜ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ")

# ğŸ“Œ 4ï¸âƒ£ CNN + LSTM ëª¨ë¸ í•™ìŠµ
model = Sequential([
    Conv1D(128, kernel_size=9, activation='relu', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),  
    MaxPooling1D(pool_size=2),
    Dropout(0.4),

    LSTM(512, return_sequences=True),
    LSTM(512, return_sequences=False),
    Dropout(0.4),

    Dense(256, activation='relu'),
    Dropout(0.4),
    Dense(len(encoder.classes_), activation='softmax')  
])

# í•™ìŠµë¥  ì¡°ì • (Adam Optimizer ê°œì„ )
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# ëª¨ë¸ í•™ìŠµ
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))

# ëª¨ë¸ í‰ê°€
loss, accuracy = model.evaluate(X_test, y_test)
print(f"ğŸ”¥ ìµœì í™”ëœ CNN + LSTM ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f}")
